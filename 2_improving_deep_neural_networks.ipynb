{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\n",
    "\n",
    "This notebook is based on my learning from **course 2** of the **Deep Learning Specialization** provided by **deeplearning.ai**. The course videos could be found on [YouTube](https://www.youtube.com/watch?v=1waHlpKiNyY&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc) or [Coursera](https://www.coursera.org/specializations/deep-learning). Learning through Coursera is highly recommended to get access to the quizes and programmin exercises along the course, as well as the course certification upon completion. Personally, I completed the specialization of 5 coursesand acquired [Specialization Certificate](https://coursera.org/share/e590c28a5c258e500ca6d3ccb4ed57ba). Later, I discovered the YouTube videos and used them for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Practical Aspects of Deep Learning\n",
    "\n",
    "### Test/Dev/Test Sets\n",
    "- Applying DL is a very iterative process to find the best hyperparameters.\n",
    "- Split dataset\n",
    "    - Previous era: 70/30 or 60/20/20 (n <= 100,000)\n",
    "    - Big data: 98/1/1 (n >= 1,000,000) 10,000 samples might be enough for test set; 99.5/ .4/ .1 for even bigger dataset\n",
    "- Beware of mismatched train/test distribution\n",
    "- It might be ok to not have the test set\n",
    "\n",
    "### Bias/Variance\n",
    "- High variance: overfitting the data (low bias in training but high bias in test)\n",
    "- High bias: underfitting the data compared to baseline (human judgement) (high bias in both training and testing)\n",
    "- High bias and high variance (high bias in training and even worse in test)\n",
    "- Low bias and low variance: a really good model (low bias in both training and test)\n",
    "\n",
    "### Basic Receip\n",
    "1. High bias? (training data performance) -> Bigger network, train longer, NN architecture search\n",
    "2. High variance? (dev set performance) -> More data, regularization, NN architecture search\n",
    "\n",
    "Bias/variance tradeoff is not always the case for DL if appropriate techiniques (ex: bigger network and more data) are selected.\n",
    "\n",
    "### Regularization (might introduce bias/viariance tradeoff)\n",
    "\n",
    "- L2 regularization is used much more often than L1 when training NN models.\n",
    "- $\\lambda$ is the regularization parameter\n",
    "- In Neural Network:\n",
    "\n",
    "$J(\\mathbf{w}, \\mathbf{b}) = \\frac{1}{m}\\sum_{i=1}^{n}L(\\hat{y},y) +$<font color='blue'>$\\frac{\\lambda}{2m}\\sum_{l=1}^{L}||\\mathbf{w}^{[l]}||^2_F$</font>\n",
    "\n",
    "<font color='blue'>$\\text{Frobenius norm (L2 norm)}: ||\\mathbf{w}^{[l]}||^2_F = \\sum_{i=1}^{n^{[l-1]}}\\sum_{i=1}^{n^{[l]}}(w^{[l]}_{ij})^2$</font>, $\\mathbf{w}^{[l]}: (n^{[l-1]}, n^{[l]})$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w^{[l]}} =  \\mathbf{dw}^{[l]}, \\mathbf{dw}^{[l]} = \\frac{1}{m}\\mathbf{dZ}^{[l]} \\mathbf{A}^{[l-1]} +$ <font color='blue'>$\\frac{\\lambda}{m}\\mathbf{w}^{[l]} $</font>\n",
    "\n",
    "- With large $\\lambda$, we are telling the model to get smaller $\\mathbf{w}$. This would encourage the training process to return a simpler model, which is like having a smoother boundary between classes if visualized. \n",
    "\n",
    "### Dropout Regularization\n",
    "\n",
    "- In each iteration, randomly eliminate nodes at each layer to get a smaller, more diminished notes.\n",
    "- Implementation - inverted dropout\n",
    "```python\n",
    "keep-prob = 0.8\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep-prob\n",
    "a3 = np.multiply(a3, d3) \n",
    "a3 /= keep-prob # Correct the expected value z in z=wa+b\n",
    "```\n",
    "#### Notes:\n",
    "    1. **After adjusting the value of a, we still train w and b properly.**\n",
    "    2. **With dropout, different set of w and b are trained in each iteration so that overall the w and b are not over-trained.**\n",
    "    3. **Matrix a is dropped out instead of w. We still have all w when training the model.**\n",
    "\n",
    "\n",
    "- Making prediction at test time\n",
    "    - Not to use drop out\n",
    "    \n",
    "- Intuition: Can't rely on any one feature, so have to spread out weights. -> Shingking the weights, similar effect of L2 regularization\n",
    "- Higher keep-prob on smaller layers; No need to use keep-prob on all layers\n",
    "- Dropout is frequently used in computer vision\n",
    "- Downside: Cost function $J$ is less well-defined\n",
    "\n",
    "#### More Regularization Methods\n",
    "- Data augmentation\n",
    "- Early stopping\n",
    "\n",
    "#### Weight Initialization\n",
    "\n",
    "With large n (notes in each layer), we want smaller w.\n",
    "\n",
    "$\\text{ReLU: Var}(\\mathbf{w}^{[l]}) = \\frac{2}{n^{[l-1]}}$\n",
    "\n",
    "$\\text{tanh: Var}(\\mathbf{w}^{[l]}) = \\sqrt{\\frac{1}{n^{[l-1]}}}$\n",
    "\n",
    "```python\n",
    "\n",
    "w_l = np.random.rand(shape) * np.sqrt(1/n_previous_l)\n",
    "```\n",
    "\n",
    "However, in practice, tuning the weight in this way is usually less important compared to other tuning techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning, Batch Normalization, and Programming Frameworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
