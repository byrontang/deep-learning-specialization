{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning\n",
    "\n",
    "This notebook is based on my learning from **course 1** of the **Deep Learning Specialization** provided by **deeplearning.ai**. The course videos could be found on [YouTube](https://www.youtube.com/watch?v=CS4cs9xVecg&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0) or [Coursera](https://www.coursera.org/specializations/deep-learning). Learning through Coursera is highly recommended to get access to the quizes and programmin exercises along the course, as well as the course certification upon completion. Personally, I completed the specialization of 5 coursesand acquired [Specialization Certificate](https://coursera.org/share/e590c28a5c258e500ca6d3ccb4ed57ba). Later, I discovered the YouTube videos and used them for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to deep learning\n",
    "\n",
    "A neuron is a function, such as sigmoid or ReLu, that transforms an input into an output. We can built a very simple model with just one neural. Neurons in **hidden layer** can also be viewed as the **(hidden) features** which human consider when making decisions.\n",
    "\n",
    "Although the success of deep learning applications has been focused on unstructured data, such as image classification, there is still a lot of short-term economic value that applying deep learning creates has been on structured data, such as online advertising or big amount of data that company owns. Therefore, when thinking about deep learning in real case, don't forget to also consider possibilities of applying it on structured data.\n",
    "\n",
    "Scales (both the size of NN and amount of labeled data) derives deep learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Networks Basics\n",
    "\n",
    "### Notation of binary classfication\n",
    "$\\mathbf{x} \\in \\mathbf{R}^{n_x}, y \\in {0, 1}$\n",
    "\n",
    "m training samples: ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})}$\n",
    "\n",
    "$m = m_{train}, m_{test} =$ # of test samples\n",
    "\n",
    "$\\mathbf{X} = \\begin{bmatrix}\n",
    "| & | &  & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | &  & | \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\mathbf{X} \\in \\mathbf{R^{n_x*m}}$ X-shape = $(n_x, m)$\n",
    "\n",
    "#### Note: The vectors of x are stacked horizontally instead of vertically, which is usually used in other classficiation models. The same applies to y as below.\n",
    "\n",
    "$\\mathbf{Y} = [y^{(1)}, y^{(2)}, ..., y^{(m)}]$\n",
    "\n",
    "$\\mathbf{Y} \\in \\mathbf{R}^{1*m}$, Y-shape = $(1,m)$\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "Given $x$, want $\\hat{y} = P(y=1|x)$\n",
    "\n",
    "$x \\in \\mathbf{R}^{n_x}$\n",
    "\n",
    "Parameters: $w \\in \\mathbf{R}^{n_x}, b \\in \\mathbf{R}$\n",
    "\n",
    "Output: $\\hat{y} = \\sigma(w^tx + b)$ \n",
    "\n",
    "Sigmoid function is applied to the output of linear regression. $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "- If $z$ is large then $\\sigma(z) \\approx \\frac{1}{1+0}=1$\n",
    "\n",
    "- If $z$ is large negative then $\\sigma(z) \\approx \\frac{1}{1+Bignum}=0$\n",
    "\n",
    "### Logistic Regression Lost Function\n",
    "$L(\\hat{y}, y) = -(y*log\\hat{y}+(1-y)*log(1-\\hat{y}))$\n",
    "\n",
    "- If $y=1, L(\\hat{y},y)=-log\\hat{y} \\leftarrow$ want $log\\hat{y}$ to be large, want $\\hat{y}$ to be large\n",
    "\n",
    "- If $y=0, L(\\hat{y},y)=-log(1-\\hat{y}) \\leftarrow$ want $log(1-\\hat{y})$ to be large, want $\\hat{y}$ to be small\n",
    "\n",
    "Cost function: $J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)}) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log\\hat{y}^{(i)}+(1-y^{(i)})log(1-\\hat{y}^{(i)})]$\n",
    "\n",
    "Want to find $w,b$ that minimize $J(w,b)$. $J(w,b)$ is a **convext function**, and this is the big reason why we use this cost function.\n",
    "\n",
    "#### [Important Concept] Logistic regression can be viewed as a small nueral network.\n",
    "\n",
    "### Gradient Descent\n",
    "Repeat {\n",
    "\n",
    "Compute predicts $\\hat{y}$ and cost function $J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}, y)$\n",
    "\n",
    "$w:=w-\\alpha\\frac{\\partial J(w, b)}{\\partial w}$ \n",
    "\n",
    "$b:=b-\\alpha\\frac{\\partial J(w, b)}{\\partial b}$ \n",
    "\n",
    "}\n",
    "\n",
    "$\\partial$ means partial derivative when there are two variables.\n",
    "\n",
    "$\\alpha$ is the learning rate and $\\frac{\\partial J(w, b)}{\\partial w}$ is the update of $w$. The derivative of the function is the slope of the function at the point. \n",
    "\n",
    "A convention of $\\frac{\\partial J(w, b)}{\\partial w}$ in later courses will be $dw$, and $\\frac{\\partial J(w, b)}{\\partial b}$ will be $db$.\n",
    "\n",
    "#### Backpropagation is the process of getting derivatives and update paramaters\n",
    "\n",
    "### Logistic regression derivatives (C1W2L09)\n",
    "\n",
    "**Computational Graph - Forwardpropagation**\n",
    "\n",
    "$\\fbox{$x_1, ..., x_n, w_1, ..., w_n$}\\rightarrow$ $\\fbox{$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b$}\\rightarrow$ $\\fbox{$a = \\sigma(z)$}\\rightarrow $$\\fbox{$L(a,y)=-(ylog(a)+(1-y)log(1-a))$}$ \n",
    "\n",
    "\n",
    "**Computational Graph - Backpropagation**\n",
    "\n",
    "#### [VERY IMPORTANT NOTE!!!]: In backpropagation, what we want to do is to - get the derivative of $z, w, \\text{and } b$ repective to  LOST $L(a,y)$ *so that we can gradually minimize the cost of lost function*. We need $da$ to compute $dz$.\n",
    "\n",
    "$\\fbox{$x_1, ..., x_n, w_1, ..., w_n$}$ <font color='green'>$\\Leftarrow$</font>  $\\fbox{$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b$}$ <font color='blue'>$\\Leftarrow$</font> $\\fbox{$a = \\sigma(z)$}$ <font color='brown'>$\\Leftarrow $</font> $\\fbox{$L(a,y)=-(ylog(a)+(1-y)log(1-a))$}$ \n",
    "\n",
    "<font color='brown'>$$da = \\frac{dL(a,y)}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}$$</font>\n",
    "\n",
    "<font color='blue'>$$dz = \\frac{dL}{dz} = \\frac{dL(a,y)}{dz} = \\frac{dL}{da}\\frac{da}{dz} = da \\times g'(z) = (-\\frac{y}{a} + \\frac{1-y}{1-a}) (a(1-a)) = a-y \\text{ (Chain Rule)}$$</font>\n",
    "\n",
    "<font color='green'>$$dw = \\frac{\\partial L}{\\partial w_n} =\\frac{dL}{da}\\frac{da}{dz}\\frac{\\partial z}{\\partial dw} = x_ndz$$</font>\n",
    "<font color='green'>$$db = \\frac{\\partial L}{\\partial b} = dz$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization for m samples & Recap\n",
    "\n",
    "#### Forwardpropagation\n",
    "\n",
    "$\\mathbf{X} = \\begin{bmatrix}\n",
    "| & | &  & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | &  & | \\\\ \n",
    "\\end{bmatrix}, \\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_n\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$$\\mathbf{Z} = [z^{(1)}, z^{(2)}, ..., z^{(m)}] = \\mathbf{w}^T\\mathbf{X}+\\mathbf{b}\n",
    "$$\n",
    "\n",
    "$$\\mathbf{A} = \\sigma(\\mathbf{Z})$$\n",
    "\n",
    "#### Backpropagation (C1W2L14)\n",
    "\n",
    "**For n samples:**\n",
    "\n",
    "$dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, ...$\n",
    "\n",
    "$\\mathbf{dZ} = [dz^{(1)}, dz^{(2)}, ..., dz^{(m)}]$\n",
    "\n",
    "$\\mathbf{A} = [a^{(1)} ... a^{(m)}], \\mathbf{Y} = [y^{(1)} ... y^{(m)}]$\n",
    "\n",
    "$$\\mathbf{dZ} = A-Y$$\n",
    "\n",
    "$$db = \\frac{1}{m}\\sum_{i=1}^{m}dz^{(i)} = \\frac{1}{m}np.sum(\\mathbf{dZ})$$\n",
    "\n",
    "$$\\mathbf{dw} = \\frac{1}{m}\\mathbf{X}\\mathbf{dZ}^T$$\n",
    "\n",
    "$$\\mathbf{w}:=\\mathbf{w}-\\alpha \\mathbf{dw}$$\n",
    "$$b:=w-\\alpha db$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant codes in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250204.84932320815\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "# vectorization\n",
    "c = np.dot(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56.    0.    4.4  68. ]\n",
      " [  1.2 104.   52.    8. ]\n",
      " [  1.8 135.   99.    0.9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[56.0, 0.0, 4.4, 68.0],\n",
    "              [1.2, 104.0, 52.0, 8.0],\n",
    "              [1.8, 135, 99.0, 0.9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 59.  239.  155.4  76.9]\n"
     ]
    }
   ],
   "source": [
    "cal = A.sum(axis=0)\n",
    "print(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94.91525424  0.          2.83140283 88.42652796]\n",
      " [ 2.03389831 43.51464435 33.46203346 10.40312094]\n",
      " [ 3.05084746 56.48535565 63.70656371  1.17035111]]\n"
     ]
    }
   ],
   "source": [
    "# broadcasting\n",
    "percentage = 100*A/cal.reshape(1,4)\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "# rank 1 array - doesn't behave like a vector\n",
    "print(np.random.randn(5).shape)\n",
    "# column vector\n",
    "print(np.random.randn(5,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 2-Layer Neural Network\n",
    "\n",
    "Recall in **logistic regression**:\n",
    "\n",
    "$$z = w^Tx + b \\rightarrow a=\\sigma(z) \\rightarrow L(a,y)$$\n",
    "\n",
    "In **neural network**, there are more layers, in which there are neurons similar to a logistic regression. The parameters correspond to the first layer of neuron will be denoted with superscript $[1]$, and so on.\n",
    "\n",
    "$$z^{[1]} = w^{[1]}x+b^{[1]} \\rightarrow a^{[1]}=g^{[1]}(z^{[1]}) \\rightarrow z^{[2]} = w^{[2]}a^{[1]}+b^{[2]} \\rightarrow a^{[2]}=g^{[2]}(z^{[2]}) \\rightarrow L(a^{[2]}, y)$$\n",
    "\n",
    "$g^{[i]}$ is the activation function of layer $i$.\n",
    "\n",
    "### Representation\n",
    "- Input layer\n",
    "    - $a^{[0]}=x$\n",
    "- Hidden layer\n",
    "    - $a^{[1]} = \\begin{bmatrix}\n",
    "                {a_1}^{[1]} \\\\\n",
    "                {a_2}^{[1]} \\\\\n",
    "                \\vdots \\\\\n",
    "                {a_n}^{[1]}\\\\ \n",
    "                \\end{bmatrix}$\n",
    "- Output layer\n",
    "\n",
    "When we count the layers we don't don't the input layer. A NN with only 1 hidden layer is called 2 layer NN.\n",
    "\n",
    "### Forwardpropagation\n",
    "\n",
    "Given input $\\mathbf{X} = \\begin{bmatrix}\n",
    "| & | &  & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | &  & | \\\\ \n",
    "\\end{bmatrix}$:                                               \n",
    "\n",
    "$$\\mathbf{Z}^{[1]} = \\mathbf{w}^{[1]}\\mathbf{X} + \\mathbf{b}^{[1]}$$\n",
    "\n",
    "$$\\mathbf{A}^{[1]} = g^{[1]}(\\mathbf{Z}^{[1]})$$\n",
    "\n",
    "$$\\mathbf{Z}^{[2]} = \\mathbf{w}^{[2]}\\mathbf{A}^{[1]} + \\mathbf{b}^{[2]}$$\n",
    "                                \n",
    "$$\\mathbf{A}^{[2]} = g^{[2]}(\\mathbf{Z}^{[2]})$$\n",
    "\n",
    "\n",
    ", where $\\mathbf{w}^{[1]} = \\begin{bmatrix}\n",
    "                            - {w_1}^{[1]T}- \\\\\n",
    "                            - {w_2}^{[1]T}- \\\\\n",
    "                            \\vdots \\\\\n",
    "                            - {w_i}^{[1]T}-\\\\ \n",
    "                            \\end{bmatrix},\n",
    "          \\mathbf{b}^{[1]} = \\begin{bmatrix}\n",
    "                            {b_1}^{[1]T}\\\\\n",
    "                            {b_2}^{[1]T}\\\\\n",
    "                            \\vdots \\\\\n",
    "                            {b_i}^{[1]T}\\\\ \n",
    "                            \\end{bmatrix},\n",
    "          \\mathbf{Z}^{[1]} = \\begin{bmatrix}\n",
    "                       | & | &  & | \\\\\n",
    "                       z^{[1](1)} & z^{[1](2)} & \\cdots & z^{[1](m)} \\\\\n",
    "                       | & | &  & | \\\\ \n",
    "                       \\end{bmatrix},\n",
    "          \\mathbf{A}^{[1]} = \\begin{bmatrix}\n",
    "                       | & | &  & | \\\\\n",
    "                       a^{[1](1)} & a^{[1](2)} & \\cdots & a^{[1](m)} \\\\\n",
    "                       | & | &  & | \\\\ \n",
    "                       \\end{bmatrix}$\n",
    "\n",
    "The shape of $\\mathbf{w}^{[1]}$ is $(n^{[1]}, n^{[0]})$, and the shape of $\\mathbf{b}^{[1]}$ is $(n^{[1]}, 1)$, where $n^{[1]} =$ the number of neurons in first layer and $^{[0]} =$ dimensions of $x$.\n",
    "\n",
    "The shape of $\\mathbf{Z}^{[1]}$ and $\\mathbf{A}^{[1]}$ is $(n^{[1]}, m)$.\n",
    "\n",
    "#### Note: $(n^{[1]}, n^{[0]}) \\times (n^{[0]}, m) = (n^{[1]}, m)$\n",
    "\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Parameters: $\\mathbf{w}^{[1]}, \\mathbf{b}^{[1]}, \\mathbf{w}^{[2]}, \\mathbf{b}^{[2]}$\n",
    "\n",
    "Cost function: $J(\\mathbf{w}^{[1]}, \\mathbf{b}^{[1]}, \\mathbf{w}^{[2]}, \\mathbf{b}^{[2]}) = \\frac{1}{m}\\sum_{i=1}^{n}L(\\hat{y},y)$\n",
    "\n",
    "#### Note: The relationship between $\\mathbf{w}^{[1]}, \\mathbf{b}^{[1]}, \\mathbf{w}^{[2]}, \\text{and } \\mathbf{b}^{[2]}$ is non-linear, so don't conceptualize it in a linear way. Therefore, the derivatives are all respective to the cost function $J$ as below. During the iteration, we really only want to update $\\mathbf{w}^{[1]}, \\mathbf{b}^{[1]}, \\mathbf{w}^{[2]}, \\text{and } \\mathbf{b}^{[2]}$ but inevitably need to calculate $\\mathbf{dZ}$ to achieve the goal (see formula).\n",
    "\n",
    "Repeat in one iteration {\n",
    "\n",
    "Compute predicts $\\mathbf{\\hat{y}}$\n",
    "\n",
    "$\\mathbf{dw}^{[1]} = \\frac{\\partial J}{\\partial \\mathbf{w}^{[1]}}, \\mathbf{db}^{[1]} = \\frac{\\partial J}{\\partial \\mathbf{b}^{[1]}}, \\mathbf{dw}^{[2]} = \\frac{\\partial J}{\\partial \\mathbf{w}^{[2]}}, \\mathbf{db}^{[2]} = \\frac{\\partial J}{\\partial \\mathbf{b}^{[2]}}$\n",
    "\n",
    "$\\mathbf{w}^{[1]}:=\\mathbf{w}^{[1]}-\\alpha\\mathbf{dw}^{[1]}, \\mathbf{b}^{[1]}:=\\mathbf{b}^{[1]}-\\alpha\\mathbf{db}^{[1]}, \\mathbf{w}^{[2]}:=\\mathbf{w}^{[2]}-\\alpha\\mathbf{dw}^{[2]}, \\mathbf{b}^{[2]}:=\\mathbf{b}^{[2]}-\\alpha\\mathbf{db}^{[2]}$ \n",
    "\n",
    "}\n",
    "\n",
    "### Formula to Compute Derivatives - Backpropagation:\n",
    "**Assuming $\\sigma()$ is used in output layer.**\n",
    "\n",
    "Refer to logistic regression for the following formula.\n",
    "$$\\mathbf{dZ}^{[2]} = A^{[2]}-Y$$\n",
    "$$\\mathbf{dw}^{[2]} = \\frac{1}{m}\\mathbf{dZ}^{[2]}\\mathbf{A}^{[1]T}$$ \n",
    "$$db^{[2]} = \\frac{1}{m}\\sum_{i=1}^{m}dz^{[2]} = \\frac{1}{m}np.sum(\\mathbf{dZ}^{[2]}, axis=1, keepdims=True)$$\n",
    "\n",
    "#### Note: The shape of $\\mathbf{dZ}^{[1]}$ formula is $(n^{[1]}, m) \\times (n^{[1]}, m)$.\n",
    "$$\\mathbf{dZ}^{[1]} = \\mathbf{w}^{[2]T}\\mathbf{dZ}^{[2]} \\times g^{[1]'}(\\mathbf{Z}^{[1]})$$\n",
    "$$\\mathbf{dw}^{[1]} = \\frac{1}{m}\\mathbf{dZ}^{[1]}\\mathbf{X^T}$$\n",
    "$$db^{[1]} = \\frac{1}{m}\\sum_{i=1}^{m}dz^{[1]} = \\frac{1}{m}np.sum(\\mathbf{dZ}^{[1]}, axis=1, keepdims=True)$$\n",
    "\n",
    "How to get $\\mathbf{dZ}^{[1]}$:\n",
    "$$\\mathbf{dZ}^{[1]} = \\frac{\\partial L}{\\mathbf{\\partial A}^{[1]}} \\times \\frac{\\mathbf{dA}^{[1]}}{\\mathbf{dZ}^{[1]}}= \\frac{\\partial L}{\\mathbf{\\partial A^{[1]}}} \\times \\frac{d}{dZ^{[1]}}g(\\mathbf{Z}^{[1]}) = \\mathbf{dA^{[1]}}\\times g'(\\mathbf{Z}^{[1]})$$\n",
    "\n",
    "$$\\mathbf{dA}^{[1]} = \\frac{dL}{\\mathbf{dZ}^{[2]}} \\times \\frac{\\mathbf{dZ}^{[2]}}{\\mathbf{dA}^{[1]}} = \\mathbf{dZ}^{[2]} \\frac{d(\\mathbf{w}^{[2]}\\mathbf{A}^{[1]}+\\mathbf{b}^{[2]})}{\\mathbf{dA}^{[1]}} = \\mathbf{dZ}^{[2]}\\mathbf{w}^{[2]}$$\n",
    "\n",
    "#### More references:\n",
    "- [Backpropagation calculas | Deep learning, chapter 3 by 3BLUE1BROWN](https://www.youtube.com/watch?v=Ilg3gGewQ5U) gives a clearer picture of backpropagation by visual representation\n",
    "- [Backpropagation calculas | Deep learning, chapter 4 by 3BLUE1BROWN](https://www.youtube.com/watch?v=tIeHLnjs5U8) talks more about the ituition of chain rules\n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Sigmoid function is one of the activation functions. A better activation function is **tanh function**, which is a mathmatically shifted function of sigmoid function. The tanh function goes through 0 and scales between -1 and +1. $$tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "\n",
    "For hidden units, if you let the actifunction $g(z)$ be equal to $tanh(z)$, this almost always works better than sigmoid function. With values between -1 and +1, the means of the activations that come out of the hidden layer are closer to have a zero mean just as sometimes we center our data when we train the model. This makes the learning of the next layer a bit eaiser. One situation to keep using sigmoid function is at the output layer for classification. However, one disadvantage of tanh function is the gradient becomes very small when z is very large or very small.\n",
    "\n",
    "One other choise that is popular for machine learning is **ReLu (rectified linear unit) function**\n",
    "$$a = max(0, z)$$\n",
    "\n",
    "One disadvantage of ReLu is that the derivative is 0 then z is negative. In practice, it works just fine as most of the hidden units will deliver z > 0. We can use leaky ReLu solves this issue, but it is less common.\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "- **Sigmoid**: Never use this unless for the output layer. \n",
    "- **Tanh**: A better option than sigmoid\n",
    "- **ReLu**: Most commonly used\n",
    "- **Leaky Relu**: An alternative of ReLu\n",
    "\n",
    "If not sure which activation function works the best, try them all and use validation set to see which one works better.\n",
    "\n",
    "### Derivatives of Activation Functions\n",
    "\n",
    "**Sigmoid: $g(z) = \\frac{1}{1+e^{-z}}$** \n",
    "\n",
    "$$\\frac{d}{dz}g(z) = \\text{slope of $g(x)$ at } z = g'(z) =\\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}}) = g(z)(1-g(z)) = a(1-a)$$\n",
    "\n",
    "**tanh: $g(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$**\n",
    "\n",
    "$$\\frac{d}{dz}g(z)=1-(tanh(z))^2 = 1-a^2$$\n",
    "\n",
    "**ReLU: $g(z) = max(0, z)$**\n",
    "\n",
    "$$\\begin{equation}\n",
    "  g'(z)=\\begin{cases}\n",
    "    0, & \\text{if $z<0$}\\\\\n",
    "    1, & \\text{if $z\\geq0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Leaky ReLU: $g(z) = max(0.01z, z)$**\n",
    "\n",
    "$$\\begin{equation}\n",
    "  g'(z)=\\begin{cases}\n",
    "    0.01, & \\text{if $z<0$}\\\\\n",
    "    1, & \\text{if $z\\geq0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Neural Networks\n",
    "\n",
    "### Notation\n",
    "- $L =$ number of layers\n",
    "- $n^{[l]} =$ number of units in layer $l$\n",
    "- $a^{[l]} =$ activations in layer $l$\n",
    "- $a^{[l]} = g^{[l]}(z^{[l]})$\n",
    "- $w^{[l]}, b^{[l]} =$ the weight and bias in layer $l$\n",
    "- $x = a^{[0]}, \\hat{y} = a^{[L]}$\n",
    "\n",
    "### Forwardpropagation\n",
    "\n",
    "for $l$ from 1 to $l$:\n",
    "\n",
    "$\\mathbf{Z}^{[l]} = \\mathbf{w}^{[l]}\\mathbf{A}^{[l-1]} + \\mathbf{b}^{[l]}$ The shape of $w^{[l]} \\text{ and } b^{[l]} : (n^{[l]}, n^{[l-1]})$ as we need to transform the dimensions from $n^{[l-1]}$ to $n^{[l]}$                     \n",
    "$\\mathbf{A}^{[l]} = g^{[l]}(\\mathbf{Z}^{[l]})$ The shape of $\\mathbf{Z}^{[l]} \\text{ and } \\mathbf{A}^{[l]} : (n^{[l]}, m)$\n",
    "\n",
    "The for loop here is inevitable.\n",
    "\n",
    "### Why Deep Representations?\n",
    "\n",
    "1. First few layers are simpler functions that learns lower-level features, and then the outputs are feeded into later layers of more complex functions to detect more complex things.\n",
    "\n",
    "2. Circuit theory and deep learning (informally): There are functions you can compute with a \"small\" L-layer deep neural network that shaoower networks require exponentially more hidden units to compute. Ex: from $O(log n)$ to $O(2^n)$\n",
    "\n",
    "3. Neural network rebranded as deep learning\n",
    "\n",
    "When starting a new questions, start with simple NN models.\n",
    "\n",
    "### Forward and backward functions\n",
    "\n",
    "large $l$: $w^{[l]}, b^{[l]}$\n",
    "\n",
    "Forward: \n",
    "- Input $A^{[l-1]}$; \n",
    "- Output $A^{[l]}$, cache $Z^{[l]}$\n",
    "\n",
    "Backward: \n",
    "- Input $dA^{[l]}$, cached $Z^{[l]}$; \n",
    "- Output: $dA^{[l-1]}, dw^{[l]}, db^{[l]}$\n",
    "\n",
    "### Back Propagation\n",
    "\n",
    "$\\mathbf{dZ}^{[l]} = \\mathbf{dA}^{[l]} \\times g^{[l]'}(\\mathbf{Z})^{[l]}$\n",
    "\n",
    "$\\mathbf{dw}^{[l]} = \\frac{1}{m}\\mathbf{dZ}^{[l]} \\mathbf{A}^{[l-1]}$\n",
    "\n",
    "$\\mathbf{db}^{[l]} = \\frac{1}{m}np.sum(\\mathbf{dZ}^{[l]}, asix=1, keepdims=True)$\n",
    "\n",
    "$\\mathbf{dA}^{[l-1]} = \\mathbf{w}^{[l]T} \\mathbf{dZ}^{[l]}$\n",
    "\n",
    "The output layer: $da^{[l]} = -\\frac{y}{a} + \\frac{1-y}{1-a}$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "$\\mathbf{w}^{[l]}:=\\mathbf{w}^{[l]}-\\alpha\\mathbf{dw}^{[l]}$ \n",
    "\n",
    "$\\mathbf{b}^{[l]}:=\\mathbf{b}^{[l]}-\\alpha\\mathbf{db}^{[l]}$ \n",
    "\n",
    "### Parameters and Hyperparameters\n",
    "\n",
    "Parameters: $\\mathbf{w}, \\mathbf{b}$\n",
    "\n",
    "Hyperparameters: the parameters (used/experimented during training) that control $\\mathbf{w}, \\mathbf{b}$\n",
    "- learning rate $\\alpha$\n",
    "- number of iteration\n",
    "- number of hidden layer $L$\n",
    "- number of hidden units $n^{[l]}$\n",
    "- choice of activation functions\n",
    "- momentum\n",
    "- minibatch size\n",
    "- regularization\n",
    "- and more\n",
    "\n",
    "Hyperparameters determins the final parameters we end up with.\n",
    "\n",
    "Try out a range of hyperparameters when starting a new problem and then tune the hyperparameters in a systematic way. Overtime, the best hyperparameters might change for the same problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
