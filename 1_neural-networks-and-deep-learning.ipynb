{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning\n",
    "\n",
    "This notebook is based on my learning from course 1 of the **Deep Learning Specialization** provided by **deeplearning.ai**. The course videos could be found on [YouTube](https://www.youtube.com/watch?v=CS4cs9xVecg&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0) or [Coursera](https://www.coursera.org/specializations/deep-learning). Learning through Coursera is highly recommended to get access to the quizes and programmin exercises along the course, as well as the course certification upon completion. Personally, I completed the specialization of 5 coursesand acquired [Specialization Certificate](https://coursera.org/share/e590c28a5c258e500ca6d3ccb4ed57ba). Later, I discovered the YouTube videos and used them for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to deep learning\n",
    "\n",
    "A neuron is a function, such as sigmoid or ReLu, that transforms an input into an output. We can built a very simple model with just one neural. Neurons in hidden layer can also be viewed as the (hidden) features which human consider when making decisions.\n",
    "\n",
    "Although the success of deep learning applications has been focused on unstructured data, such as image classification, there is still a lot of short-term economic value that applying deep learning creates has been on structured data, such as online advertising or big amount of data that company owns. Therefore, when thinking about deep learning in real case, don't forget to also consider possibilities of applying it on structured data.\n",
    "\n",
    "Scales (both the size of NN and amount of labeled data) derives deep learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Basics\n",
    "\n",
    "### Notation of binary classfication\n",
    "$\\mathbf{x} \\in \\mathbf{R}^{n_x}, y \\in {0, 1}$\n",
    "\n",
    "m training samples: ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})}$\n",
    "\n",
    "$m = m_{train}, m_{test} =$ # of test samples\n",
    "\n",
    "$\\mathbf{X} = \\begin{bmatrix}\n",
    "| & | &  & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | &  & | \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\mathbf{X} \\in \\mathbf{R^{n_x*m}}$ X-shape = $(n_x, m)$\n",
    "\n",
    "#### Note: The vectors of x are stacked horizontally instead of vertically, which is usually used in other classficiation models. The same applies to y as below.\n",
    "\n",
    "$\\mathbf{Y} = [y^{(1)}, y^{(1)}, ..., y^{(1)}]$\n",
    "\n",
    "$\\mathbf{Y} \\in \\mathbf{R}^{1*m}$, Y-shape = $(1,m)$\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "Given $x$, want $\\hat{y} = P(y=1|x)$\n",
    "\n",
    "$x \\in \\mathbf{R}^{n_x}$\n",
    "\n",
    "Parameters: $w \\in \\mathbf{R}^{n_x}, b \\in \\mathbf{R}$\n",
    "\n",
    "Output: $\\hat{y} = \\sigma(w^tx + b)$ \n",
    "\n",
    "Sigmoid function is applied to the output of linear regression. $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "- If $z$ is large then $\\sigma(z) \\approx \\frac{1}{1+0}=1$\n",
    "\n",
    "- If $z$ is large negative then $\\sigma(z) \\approx \\frac{1}{1+Bignum}=0$\n",
    "\n",
    "### Logistic Regression Lost Function\n",
    "$L(\\hat{y}, y) = -(y*log\\hat{y}+(1-y)*log(1-\\hat{y}))$\n",
    "\n",
    "- If $y=1, L(\\hat{y},y)=-log\\hat{y} \\leftarrow$ want $log\\hat{y}$ to be large, want $\\hat{y}$ to be large\n",
    "\n",
    "- If $y=0, L(\\hat{y},y)=-log(1-\\hat{y}) \\leftarrow$ want $log(1-\\hat{y})$ to be large, want $\\hat{y}$ to be small\n",
    "\n",
    "Cost function: $J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)}) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}*log\\hat{y}^{(i)}+(1-y^{(i)})*log(1-\\hat{y}^{(i)})]$\n",
    "\n",
    "Want to find $w,b$ that minimize $J(w,b)$. $J(w,b)$ is a **convext function**, and this is the big reason why we use this cost function.\n",
    "\n",
    "#### [Important Concept] Logistic regression can be viewed as a small nueral network.\n",
    "\n",
    "### Gradient Descent\n",
    "Repeat {\n",
    "\n",
    "$w:=w-\\alpha\\frac{\\partial J(w, b)}{\\partial w}$ \n",
    "\n",
    "$b:=b-\\alpha\\frac{\\partial J(w, b)}{\\partial b}$ \n",
    "\n",
    "}\n",
    "\n",
    "$\\partial$ means partial derivative when there are two variables.\n",
    "\n",
    "$\\alpha$ is the learning rate and $\\frac{\\partial J(w, b)}{\\partial w}$ is the update of $w$. The derivative of the function is the slope of the function at the point. \n",
    "\n",
    "A convention of $\\frac{\\partial J(w, b)}{\\partial w}$ in later courses will be $dw$, and $\\frac{\\partial J(w, b)}{\\partial b}$ will be $db$.\n",
    "\n",
    "#### Backpropagation is the process of getting derivatives and update paramaters\n",
    "\n",
    "### Logistic regression derivatives (C1W2L09)\n",
    "\n",
    "**For one sample:**\n",
    "\n",
    "$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b$ $\\rightarrow$ $a = \\sigma(z)$ $\\rightarrow$ $L(a,y)$ \n",
    "\n",
    "$$\\frac{dL(a,y)}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}$$\n",
    "\n",
    "$$dz = \\frac{dL}{dz} = \\frac{dL(a,y)}{dz} = \\frac{dL}{da}\\frac{da}{dz} = (-\\frac{y}{a} + \\frac{1-y}{1-a}) (a(1-a)) = a-y$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_n} = x_ndz, \\frac{\\partial L}{\\partial b} = dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization for m samples & Recap\n",
    "\n",
    "#### Forwardpropagation\n",
    "\n",
    "$\\mathbf{X} = \\begin{bmatrix}\n",
    "| & | &  & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | &  & | \\\\ \n",
    "\\end{bmatrix}, \\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_n\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$$\\mathbf{Z} = [z^{(1)}, z^{(2)}, ..., z^{(m)}] = \\mathbf{w}^T\\mathbf{X}+\\mathbf{b}\n",
    "$$\n",
    "\n",
    "$$\\mathbf{A} = \\sigma(\\mathbf{Z})$$\n",
    "\n",
    "#### Backpropagation (C1W2L14)\n",
    "\n",
    "**For n samples:**\n",
    "\n",
    "$dz^{(1)} = a^{(1)} - y^{(1)}, dz^{(2)} = a^{(2)} - y^{(2)}, ...$\n",
    "\n",
    "$\\mathbf{dZ} = [dz^{(1)}, dz^{(2)}, ..., dz^{(m)}]$\n",
    "\n",
    "$\\mathbf{A} = [a^{(1)} ... a^{(m)}], \\mathbf{Y} = [y^{(1)} ... y^{(m)}]$\n",
    "\n",
    "$$\\mathbf{dZ} = A-Y$$\n",
    "\n",
    "$$db = \\frac{1}{m}\\sum_{i=1}^{m}dz^{(i)} = \\frac{1}{m}np.sum(\\mathbf{dZ})$$\n",
    "\n",
    "$$\\mathbf{dw} = \\frac{1}{m}\\mathbf{X}\\mathbf{dZ}^T$$\n",
    "\n",
    "$$\\mathbf{w}:=\\mathbf{w}-\\alpha \\mathbf{dw}$$\n",
    "$$b:=w-\\alpha db$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant codes in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250204.84932320815\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "# vectorization\n",
    "c = np.dot(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56.    0.    4.4  68. ]\n",
      " [  1.2 104.   52.    8. ]\n",
      " [  1.8 135.   99.    0.9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[56.0, 0.0, 4.4, 68.0],\n",
    "              [1.2, 104.0, 52.0, 8.0],\n",
    "              [1.8, 135, 99.0, 0.9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 59.  239.  155.4  76.9]\n"
     ]
    }
   ],
   "source": [
    "cal = A.sum(axis=0)\n",
    "print(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94.91525424  0.          2.83140283 88.42652796]\n",
      " [ 2.03389831 43.51464435 33.46203346 10.40312094]\n",
      " [ 3.05084746 56.48535565 63.70656371  1.17035111]]\n"
     ]
    }
   ],
   "source": [
    "# broadcasting\n",
    "percentage = 100*A/cal.reshape(1,4)\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "# rank 1 array - doesn't behave like a vector\n",
    "print(np.random.randn(5).shape)\n",
    "# column vector\n",
    "print(np.random.randn(5,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network\n",
    "\n",
    "Recall in **logistic regression**:\n",
    "\n",
    "$$z = w^Tx + b \\rightarrow a=\\sigma(z) \\rightarrow L(a,y)$$\n",
    "\n",
    "In **neural network**, there are more layers, in which there are neurons similar to a logistic regression. The parameters correspond to the first layer of neuron will be denoted with superscript $[1]$, and so on.\n",
    "\n",
    "$$z^{[1]} = w^{[1]}x+b^{[1]} \\rightarrow a^{[1]}=\\sigma(z^{[1]}) \\rightarrow z^{[2]} = w^{[2]}a^{[1]}+b^{[2]} \\rightarrow a^{[2]}=\\sigma(z^{[2]}) \\rightarrow L(a^{[2]}, y)$$\n",
    "\n",
    "### Neural Network Representation\n",
    "- Input layer\n",
    "    - $a^{[0]}=x$\n",
    "- Hidden layer\n",
    "    - $a^{[1]} = \\begin{bmatrix}\n",
    "                {a_1}^{[1]} \\\\\n",
    "                {a_2}^{[1]} \\\\\n",
    "                \\vdots \\\\\n",
    "                {a_i}^{[1]}\\\\ \n",
    "                \\end{bmatrix}$\n",
    "- Output layer\n",
    "\n",
    "When we count the layers we don't don't the input layer. A NN with only 1 hidden layer is called 2 layer NN.\n",
    "\n",
    "**2 layer NN:**\n",
    "\n",
    "Given input $\\mathbf{X} = \\begin{bmatrix}\n",
    "| & | &  & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | &  & | \\\\ \n",
    "\\end{bmatrix}$:                                               \n",
    "\n",
    "$$\\mathbf{Z^{[1]}} = \\mathbf{w^{[1]}}\\mathbf{X} + \\mathbf{b^{[1]}}$$\n",
    "\n",
    "$$\\mathbf{A^{[1]}} = \\sigma(\\mathbf{Z^{[1]}})$$\n",
    "\n",
    "$$\\mathbf{Z^{[2]}} = \\mathbf{w^{[2]}}\\mathbf{A^{[1]}} + \\mathbf{b^{[2]}}$$\n",
    "                                \n",
    "$$\\mathbf{A^{[2]}} = \\sigma(\\mathbf{Z^{[2]}})$$\n",
    "\n",
    "\n",
    ", where $\\mathbf{w^{[1]}} = \\begin{bmatrix}\n",
    "                            - {w_1}^{[1]T}- \\\\\n",
    "                            - {w_2}^{[1]T}- \\\\\n",
    "                            \\vdots \\\\\n",
    "                            - {w_i}^{[1]T}-\\\\ \n",
    "                            \\end{bmatrix},\n",
    "          \\mathbf{b^{[1]}} = \\begin{bmatrix}\n",
    "                            {b_1}^{[1]T}\\\\\n",
    "                            {b_2}^{[1]T}\\\\\n",
    "                            \\vdots \\\\\n",
    "                            {b_i}^{[1]T}\\\\ \n",
    "                            \\end{bmatrix},\n",
    "          \\mathbf{Z^{[1]}} = \\begin{bmatrix}\n",
    "                       | & | &  & | \\\\\n",
    "                       z^{[1](1)} & z^{[1](2)} & \\cdots & z^{[1](m)} \\\\\n",
    "                       | & | &  & | \\\\ \n",
    "                       \\end{bmatrix},\n",
    "          \\mathbf{A^{[1]}} = \\begin{bmatrix}\n",
    "                       | & | &  & | \\\\\n",
    "                       a^{[1](1)} & a^{[1](2)} & \\cdots & a^{[1](m)} \\\\\n",
    "                       | & | &  & | \\\\ \n",
    "                       \\end{bmatrix}$\n",
    "\n",
    "The shape of $\\mathbf{w^{[1]}}$ is $(i, n)$, and the shape of $\\mathbf{b^{[1]}}$ is $(i, 1)$, where $i =$ the number of neurons in first layer and $n =$ dimensions of $x$.\n",
    "\n",
    "The shape of $\\mathbf{Z^{[1]}}$ and $\\mathbf{A^{[1]}}$ is $(i, m)$.\n",
    "\n",
    "#### Note: $(i, n) \\times (n, m) = (i, m)$\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Sigmoid function one of the activation functions. A better activation function is **tanh function**, which is a mathmatically shifted function of sigmoid function. The tanh function goes through 0 and scales between -1 and +1. $$tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "\n",
    "For hidden units, if you let the actifunction $g(z)$ be equal to $tanh(z)$, this almost always works better than sigmoid function. With values between -1 and +1, the means of the activations that come out of the hidden layer are closer to have a zero mean just as sometimes we center our data when we train the model. This makes the learning of the next layer a bit eaiser. One situation to keep using sigmoid function is at the output layer for classification. However, one disadvantage of tanh function is the gradient becomes very small when z is very large or very small.\n",
    "\n",
    "One other choise that is popular for machine learning is **ReLu (rectified linear unit) function**\n",
    "$$a = max(0, z)$$\n",
    "\n",
    "One disadvantage of ReLu is that the derivative is 0 then z is negative. In practice, it works just fine as most of the hidden units will deliver z > 0. We can use leaky ReLu solves this issue, but it is less common.\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "- **Sigmoid**: Never use this unless for the output layer. \n",
    "- **Tanh**: A better option than sigmoid\n",
    "- **ReLu**: Most commonly used\n",
    "- **Leaky Relu**: An alternative of ReLu\n",
    "\n",
    "If not sure which activation function works the best, try them all and use validation set to see which one works better.\n",
    "\n",
    "### Derivatives of Activation Functions\n",
    "\n",
    "**Sigmoid: $g(z) = \\frac{1}{1+e^{-z}}$** \n",
    "\n",
    "$$\\frac{d}{dz}g(z) = \\text{slope of $g(x)$ at } z = g'(z) =\\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}}) = g(z)(1-g(z)) = a(1-a)$$\n",
    "\n",
    "**tanh: $g(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$**\n",
    "\n",
    "$$\\frac{d}{dz}g(z)=1-(tanh(z))^2 = 1-a^2$$\n",
    "\n",
    "**ReLU: $g(z) = max(0, z)$**\n",
    "\n",
    "$$\\begin{equation}\n",
    "  g'(z)=\\begin{cases}\n",
    "    0, & \\text{if $z<0$}\\\\\n",
    "    1, & \\text{if $z\\geq0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Leaky ReLU: $g(z) = max(0.01z, z)$**\n",
    "\n",
    "$$\\begin{equation}\n",
    "  g'(z)=\\begin{cases}\n",
    "    0.01, & \\text{if $z<0$}\\\\\n",
    "    1, & \\text{if $z\\geq0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
